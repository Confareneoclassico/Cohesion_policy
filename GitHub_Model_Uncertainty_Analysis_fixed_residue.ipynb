{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the relevant libraries and check the installed package versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seaborn==0.10.1\n",
      "scipy==1.5.0\n",
      "pandas==1.0.5\n",
      "numpy==1.18.5\n",
      "matplotlib==3.2.2\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import collections as mc\n",
    "import pkg_resources\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import sobol_seq\n",
    "import time\n",
    "import types\n",
    "\n",
    "def get_imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            \n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "            \n",
    "        poorly_named_packages = {\n",
    "            \"PIL\": \"Pillow\",\n",
    "            \"sklearn\": \"scikit-learn\"\n",
    "        }\n",
    "        if name in poorly_named_packages.keys():\n",
    "            name = poorly_named_packages[name]\n",
    "\n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "for rq in requirements:\n",
    "    print(\"{}=={}\".format(*rq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the EC payments database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Database_Final_UPD_2020_corr.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the series of quasi-random numbers for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quasiRandom_df = pd.DataFrame(sobol_seq.i4_sobol_generate(4,2**17))\n",
    "\n",
    "DistributionFiMax = 0.8+quasiRandom_df[0]*0.2\n",
    "DistributionFiMin = 0.2+quasiRandom_df[1]*0.2\n",
    "\n",
    "Residue_selector = quasiRandom_df[2].round(0).astype(int)\n",
    "\n",
    "inputs = pd.concat([DistributionFiMax, DistributionFiMin, Residue_selector],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearrange the dataframe for our convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb = df[['ProgrammingPeriod','Country','NUTS1Code','NUTS2Code','Year','CF_TOTAL','EAGGF','ERDF_TOTAL','ESF']].copy()\n",
    "df2 = dfb.melt(id_vars=['ProgrammingPeriod','Country','NUTS1Code','NUTS2Code','Year'],var_name='FundingScheme')\n",
    "df2b = df2[df2.Year<2018]\n",
    "df3 = df2.pivot_table(index=['ProgrammingPeriod','FundingScheme','Country','NUTS1Code','NUTS2Code'],columns='Year', values='value')\n",
    "df4 = df3.dropna(how='all').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the slice corresponding to the set we'll be working on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4b = df4.loc[('2007-2013','ERDF_TOTAL')]\n",
    "\n",
    "df4b.loc[pd.IndexSlice[:,:,'ITH1'],:]=df4b.loc[pd.IndexSlice[:,:,'ITH1'],:].values+\\\n",
    "df4b.loc[pd.IndexSlice[:,:,'ITH2'],:].values\n",
    "df4b.loc[pd.IndexSlice[:,:,'ITH1'],:]\n",
    "df4b=df4b.reset_index()\n",
    "df4b.NUTS2Code[df4b.NUTS2Code=='ITH1']='ITH0'\n",
    "df4b = df4b[df4b.NUTS2Code != 'ITH2']\n",
    "df4b=df4b.set_index(['Country','NUTS1Code','NUTS2Code']).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding years before the commencing of the programming periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = []\n",
    "\n",
    "diff = int(np.sort(np.array(list(set(df.ProgrammingPeriod))))[0][5:])+6-\\\n",
    "int(np.sort(np.array(list(set(df.ProgrammingPeriod))))[0][:4])\n",
    "\n",
    "d_Var = pd.Series([k/diff for k in range(1,diff+1)],[y for y in \\\n",
    "range(int(np.sort(np.array(list(set(df.ProgrammingPeriod))))[0][:4]),\n",
    "int(np.sort(np.array(list(set(df.ProgrammingPeriod))))[0][5:])+6)])\n",
    "\n",
    "dummy.append(d_Var)\n",
    "\n",
    "for pp in np.sort(np.array(list(set(df.ProgrammingPeriod))))[1:-1]:\n",
    "    diff = int(pp[5:])+4-int(pp[:4])\n",
    "    \n",
    "    d_Var = pd.Series([k/diff for k in range(1,diff+1)],[y for y in range(int(pp[:4]),int(pp[5:])+4)])\n",
    "    \n",
    "    dummy.append(d_Var)\n",
    "    \n",
    "for pp in np.sort(np.array(list(set(df.ProgrammingPeriod))))[-1:]:\n",
    "    diff = int(pp[5:])+5-int(pp[:4])\n",
    "    \n",
    "    d_Var = pd.Series([k/diff for k in range(1,diff+1)],[y for y in range(int(pp[:4]),int(pp[5:])+5)])\n",
    "    \n",
    "    dummy.append(d_Var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise the cumulative figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4b.copy()\n",
    "\n",
    "Norm_df6 = ((df5.cumsum(axis=1).T/df5.cumsum(axis=1).max(axis=1).values).T).dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the outcome variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Norm_df6['$\\phi$']=(Norm_df6.loc[:,[y for y in range(2007,2018)]]-dummy[-1]).cumsum(axis=1).iloc[:,-1]\n",
    "\n",
    "Norm_df6['$\\mu$']=(Norm_df6['$\\phi$'].max()-Norm_df6['$\\phi$'])/(Norm_df6['$\\phi$'].max()-Norm_df6['$\\phi$'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the trigger for the number of years the residual expenditure gets spread onto on the last eligible year of the programming period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld1 = []\n",
    "for iq,qr in enumerate(quasiRandom_df[3]):\n",
    "    df8b = Norm_df6.copy()\n",
    "    df8b[0]=(qr*(Norm_df6['$\\mu$']*(len(dummy[-1])-1)).astype(int)).astype(int)+1\n",
    "    for il in range(1,len(dummy[-1])):\n",
    "        df8b[il]=df8b[0]-il\n",
    "    df8b[df8b<1]=1\n",
    "    cd = [il0 for il0 in range(len(dummy[-1]))]\n",
    "    df8b['value']=iq\n",
    "    cd.append('value')\n",
    "    df8b = df8b[cd]\n",
    "    ld1.append(df8b)\n",
    "years = pd.concat(ld1)\n",
    "years.set_index('value', append=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the yearly residues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A(n):\n",
    "    return [2**j/(2**n-1) for j in reversed(range(n))]\n",
    "\n",
    "B9 = []\n",
    "for k in reversed(range(2,12)):\n",
    "    B9.append(pd.DataFrame([A(y) for y in range(1,k)],index=[y for y in range(1,k)],\n",
    "                           columns=[y for y in range(1,k)]).fillna(0).sort_values(by=1,ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And the number of years from which the Excess get taken out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C(n):\n",
    "    return [1/n for j in reversed(range(n))]\n",
    "\n",
    "C9 = pd.DataFrame([C(y) for y in range(1,11)],index=[y for y in range(1,11)],\n",
    "                           columns=[y for y in range(1,11)]).fillna(0).sort_values(by=1,ascending=False)\n",
    "\n",
    "D = [C9,pd.DataFrame([A(y) for y in range(1,11)],index=[y for y in range(1,11)],\n",
    "                           columns=[y for y in range(1,11)]).fillna(0).sort_values(by=1,ascending=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the EC payment slice for Italy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Payments = df4b.loc['IT'].iloc[:,-12:].copy()\n",
    "Payments.index=Payments.index.droplevel(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the member-state figure our data will be compared against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel('20181231 Pagamenti ammessi PO 2007-2013.xls',usecols=[0,1,2,3,4,5,7])\n",
    "df_REGIO_capped = df1[(df1['CCI'].str.contains(\"161\"))|df1['CCI'].str.contains(\"162\")]\n",
    "NUTS2_dic = {'ABRUZZO':'ITF1','BASILICATA':'ITF5','CALABRIA':'ITF6','CAMPANIA':'ITF3','EMILIA':'ITH5','FRIULI':'ITH4','LAZIO':'ITI4',\n",
    "'LIGURIA':'ITC3','LOMBARDIA':'ITC4','MARCHE':'ITI3','MOLISE':'ITF2','PIEMONTE':'ITC1','PUGLIA':'ITF4','SARDEGNA':'ITG2','SICILIA':'ITG1',\n",
    "'TOSCANA':'ITI1','TRENTINO':'ITH0','UMBRIA':'ITI2',\"VALLE D'AOSTA\":'ITC2','VENETO':'ITH3'}\n",
    "\n",
    "df_REGIO_capped['NUTS2'] = df_REGIO_capped.REGIONE.map(NUTS2_dic)\n",
    "\n",
    "df_REGIO_capped.DESCRIZIONE_PROGRAMMA = df_REGIO_capped.DESCRIZIONE_PROGRAMMA.str.replace(' Romagna', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute the orphan payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REGIO_capped.loc[df_REGIO_capped['NUTS2'].isnull(),'NUTS2'] = \\\n",
    "df_REGIO_capped['DESCRIZIONE_PROGRAMMA'].str.split(expand=True)[3].map(NUTS2_dic)\n",
    "\n",
    "df_REGIO_nonAttributed = df_REGIO_capped[df_REGIO_capped.NUTS2.isna()]\n",
    "df_REGIO_REGIO = df_REGIO_capped.dropna()\n",
    "\n",
    "df_REGIO_REGIO=df_REGIO_REGIO.rename(columns={'ANNO':'Year'})\n",
    "\n",
    "df_REGIO_REGIO_yearly=df_REGIO_REGIO.groupby(['Year','NUTS2']).sum()\n",
    "df_REGIO_REGIO_yearly['Year']=df_REGIO_REGIO_yearly.index.get_level_values(0)\n",
    "df_REGIO_REGIO_yearly=df_REGIO_REGIO_yearly\n",
    "df_REGIO_REGIO_yearly.index= df_REGIO_REGIO_yearly.index.droplevel(0)\n",
    "\n",
    "df_REGIO_pv = df_REGIO_REGIO_yearly.pivot_table(index='Year', columns='NUTS2', values='PAGAMENTO_AMMESSO_UE').fillna(0).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-distribute the residues as per the NUTS2 expenditures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REGIO_multiplier_year=df_REGIO_nonAttributed.groupby('ANNO').sum().T.values*df_REGIO_pv/df_REGIO_pv.sum()\n",
    "\n",
    "df_REGIO_multiplier_year['r']=1\n",
    "\n",
    "df_REGIO_multiplier = df_REGIO_nonAttributed.groupby('ANNO').sum().T.values*\\\n",
    "pd.concat([df_REGIO_pv.sum(axis=1)/df_REGIO_pv.sum(axis=1).sum() for re in range(12)],axis=1)\n",
    "df_REGIO_multiplier['r']=0\n",
    "df_REGIO_multiplier.columns = df_REGIO_multiplier_year.columns\n",
    "\n",
    "df_REGIO_residual = pd.concat([df_REGIO_multiplier,df_REGIO_multiplier_year])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the distance between the modelled expenditure and the MS incurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expe = pd.concat([Payments.copy() for r in range(len(quasiRandom_df))])\n",
    "\n",
    "N_df = pd.concat([Norm_df6.loc['IT','$\\mu$'] for r in range(len(quasiRandom_df))])\n",
    "N_df.index = N_df.index.droplevel(0) \n",
    "\n",
    "Est_expenditure = pd.concat([df_REGIO_pv.copy() for r in range(len(quasiRandom_df))])\n",
    "\n",
    "Exp = Expe.copy()\n",
    "\n",
    "Exp['r']=np.array([r for r in range(len(quasiRandom_df)) for er in range(len(Payments))])\n",
    "\n",
    "Exp.loc[:,2017]=Expe.loc[:,[y for y in range(2017,2019)]].sum(axis=1)\n",
    "Exp.loc[:,[y for y in range(2018,2019)]]=0\n",
    "\n",
    "Aggregate = Exp.loc[:,2017]*(1-(DistributionFiMax[Exp.r].values-N_df*(DistributionFiMax[Exp.r]-\\\n",
    "                                                                      DistributionFiMin[Exp.r]).values))\n",
    "\n",
    "Exp.loc[:,2017] = Exp.loc[:,2017]*(DistributionFiMax[Exp.r].values-N_df*\n",
    "                            (DistributionFiMax[Exp.r]-DistributionFiMin[Exp.r]).values)\n",
    "\n",
    "y_slice = years.loc[pd.IndexSlice['IT',:,:]]\n",
    "y_slice.index = y_slice.index.droplevel(0)\n",
    "\n",
    "\n",
    "for iy2,y2 in enumerate(reversed(range(2007,2017))):\n",
    "    Exp[y2] = Expe[y2]*(DistributionFiMax[Exp.r].values-N_df*(DistributionFiMax[Exp.r]-DistributionFiMin[Exp.r]).values)\n",
    "\n",
    "    for iy3,y3 in enumerate(reversed(range(y2,2016))):\n",
    "        Exp[y2]+=Expe[y3+1]*(1-DistributionFiMax[Exp.r].values+N_df*(DistributionFiMax[Exp.r]-\\\n",
    "                DistributionFiMin[Exp.r]).values)*B9[iy3].loc[y_slice.iloc[:,iy3].values,(y3+1-y2)].values\n",
    "\n",
    "    Exp[y2] += Aggregate*B9[0].loc[y_slice.iloc[:,0],iy2+1].values\n",
    "\n",
    "Exp[2007] += Expe[2007]*(1-DistributionFiMax[Exp.r].values+N_df*(DistributionFiMax[Exp.r]-DistributionFiMin[Exp.r]).values)\n",
    "Exp = Exp.drop('r',axis=1)\n",
    "\n",
    "Distance_fixed_residual = []\n",
    "\n",
    "for ie in range(2):\n",
    "    residual = pd.concat(df_REGIO_residual.loc[df_REGIO_residual.r==ie] for rs in Residue_selector)\n",
    "\n",
    "    MS_Expenditure = (Est_expenditure+residual).drop('r',axis=1)\n",
    "\n",
    "    Distance = pd.DataFrame((np.abs(MS_Expenditure.cumsum(axis=1)-Exp.cumsum(axis=1)).sum(axis=1)/\\\n",
    "                             Expe.cumsum(axis=1).iloc[:,-1])).rename(columns={2016:'Distance'})\n",
    "\n",
    "    Distance['r']=np.array([r for r in range(len(quasiRandom_df)) for er in range(len(Payments))])\n",
    "\n",
    "    Distance_df = Distance.pivot_table(values=0,index='r',columns='NUTS2')\n",
    "    \n",
    "    Distance_fixed_residual.append(Distance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_code = dict(zip(Payments.index, ['Pdm','AsV','Lgr','Lmb','Abr','Mls','Cmp','Apl','Bsl','Clb','Scl','Srd','Vnt','FVG','EmR','TST','Tsc',\n",
    "                 'Umb','Mrc','Laz']))\n",
    "\n",
    "Dfs = Distance_fixed_residual.copy()\n",
    "for d in Dfs:\n",
    "    d.loc['code'] = d.columns.map(dict_code.get)\n",
    "    d.columns = d.loc['code']\n",
    "    \n",
    "final_comparison = pd.concat([pd.concat(Dfs,axis=1).iloc[:-1], \n",
    "                              pd.read_excel('2020-06-15_distance_modelled_observed_expenditures.xlsx', index_col=0)],\n",
    "                             axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty analysis - distribution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in final_comparison.columns.get_level_values(0).unique():\n",
    "    plt.figure(figsize=(16, 10), dpi= 300, facecolor='w', edgecolor='k')\n",
    "    ls = [y for y in range(1,len(final_comparison[c].columns))]\n",
    "    final_comparison[c].plot(kind='box')\n",
    "    ls.extend(['Var'])\n",
    "    plt.xticks(np.arange(1,len(final_comparison[c].columns)+1),\n",
    "              tuple(ls),fontsize = 18)\n",
    "    plt.ylabel('Distance',fontsize = 24)\n",
    "    plt.xlabel('Residual',fontsize = 24)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "#     plt.savefig(str(c)+'_residual_fixing.png', dpi= 300)\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
